Large language models are increasingly converging in their outputs---different models produce remarkably similar responses to the same prompts. Chain of Thought (CoT) prompting is now ubiquitous in modern LLMs, yet its specific effect on this convergence phenomenon has never been directly measured. We conduct the first empirical study of CoT's effect on both within-model and cross-model output convergence, prompting four LLMs from different families (\gptfour, \claude, \gemini, \llama) with and without CoT across 46 questions spanning reasoning, creative, and opinion tasks. We find that CoT dramatically increases cross-model convergence (Cohen's $d = 0.93$, $p < 0.00001$), raising average pairwise cosine similarity from 0.634 to 0.832. However, CoT does \emph{not} increase within-model convergence ($d = -0.074$), meaning individual models maintain similar diversity across repeated samples. This asymmetry reveals that CoT acts as a \emph{cross-model homogenizer}: it funnels different architectures toward shared reasoning templates while preserving each model's individual variability. The effect is strongest for reasoning tasks ($d = 1.19$) and persists across creative ($d = 0.76$) and opinion ($d = 0.90$) tasks. These findings have direct implications for AI deployment strategies that rely on model diversity, benchmark design, and the growing concern over LLM output homogenization.