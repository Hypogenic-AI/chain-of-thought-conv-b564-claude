\section{Introduction}
\label{sec:introduction}

Large language models are converging. Despite differences in architecture, training data, and alignment procedures, models from OpenAI, Anthropic, Google, and Meta produce increasingly similar outputs to the same prompts~\citep{wenger2025creative,mirka2025homogenizing}. This homogenization threatens the value of maintaining multiple AI systems---if all models give essentially the same answer, the benefits of model diversity for creative applications, cultural pluralism, and robust decision-making are diminished.

At the same time, Chain of Thought (CoT) prompting has become ubiquitous.  Since \citet{wei2022chain} showed that prompting models to ``think step by step'' dramatically improves reasoning, CoT has been adopted across virtually every LLM application and has been integrated into model training itself~\citep{chen2025reasoning}. Yet while CoT's effect on \emph{accuracy} is well-studied, its effect on output \emph{diversity} across models is not. Recent work provides suggestive evidence: \citet{xu2026language} showed that English-language CoT creates ``convergence basins'' in thinking space within individual models, and \citet{jain2025homogenization} demonstrated that reasoning tasks achieve only 2--3 distinct solution strategies even with diversity-promoting techniques. However, no study has directly measured whether CoT increases or decreases the similarity of outputs \emph{across different model families}.

We conduct the first direct empirical test. We prompt four LLMs from different families---\gptfour (OpenAI), \claude (Anthropic), \gemini (Google), and \llama (Meta)---with and without CoT on 46 questions spanning three task types: reasoning (BIG-Bench Hard), creative writing (NoveltyBench), and opinion expression (World Values Survey). For each model--condition--question combination, we generate three responses at temperature 0.7 and measure convergence using pairwise cosine similarity of sentence embeddings at two levels: within-model (how similar are repeated samples from the same model?) and cross-model (how similar are responses from different models?).

Our results reveal a striking asymmetry. CoT increases cross-model convergence by 31\% in relative terms (pairwise cosine similarity from 0.634 to 0.832), with a large effect size (Cohen's $d = 0.93$, $p < 0.00001$). But CoT does \emph{not} increase within-model convergence ($d = -0.074$). In other words, CoT makes different models sound alike without making any individual model more repetitive. The effect is strongest for reasoning tasks ($d = 1.19$) and persists across creative ($d = 0.76$) and opinion tasks ($d = 0.90$). We also find a surprising dissociation: CoT \emph{decreases} answer agreement on reasoning tasks (from 44.8\% to 22.9\%) despite dramatically increasing semantic similarity---models produce similar-looking reasoning chains that sometimes lead to different conclusions.

These findings have direct implications for AI deployment, benchmark design, and the ongoing discussion around LLM homogenization. Organizations that maintain multiple model providers for diversity should be aware that CoT significantly reduces the effective diversity of their model ensemble. Benchmarks that require CoT may underestimate true inter-model differences. And the convergence phenomenon itself appears to be driven, at least in part, by a specific and modifiable prompting practice rather than being an inevitable consequence of scale.

We make the following contributions:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item We provide the first direct empirical evidence that CoT increases cross-model convergence (Cohen's $d = 0.93$) while not increasing within-model convergence, identifying CoT as a cross-model homogenizer.
    \item We demonstrate that this effect holds across reasoning, creative, and opinion tasks, with the strongest effect on reasoning ($d = 1.19$), and identify a dissociation between semantic similarity and answer agreement under CoT.
    \item We discuss implications for model diversity strategies and benchmark design, and identify response length and shared training data as potential mechanisms warranting further investigation.
\end{itemize}