\section{Related Work}
\label{sec:related_work}

\para{Chain of Thought prompting.}
\citet{wei2022chain} introduced CoT prompting by demonstrating that including intermediate reasoning steps in few-shot exemplars unlocks reasoning abilities in large language models. This approach has been extended to zero-shot settings~\citep{kojima2022large}, self-consistency decoding~\citep{wang2023selfconsistency}, and has been incorporated directly into model training via outcome-based reinforcement learning~\citep{chen2025reasoning}. While the accuracy benefits of CoT are well-established, its effects on output diversity and cross-model similarity have received far less attention. Our work addresses this gap by directly measuring how CoT changes the similarity structure of outputs across model families.

\para{LLM output homogenization.}
A growing body of work documents that LLM outputs are converging across model families. \citet{wenger2025creative} found that 22 LLMs from different families show dramatically lower creative variability than humans (effect sizes 1.4--2.2), even when controlling for response structure. \citet{mirka2025homogenizing} provided a comprehensive review of LLM-driven content homogenization across research ideation, essay writing, survey responses, and creative tasks. \citet{jain2025homogenization} showed that the degree of homogenization is task-dependent, with problem-solving tasks achieving only 2--3 distinct strategies out of 5 generations even with diversity-promoting techniques. Unlike these studies, which document homogenization as a phenomenon, we test whether a specific prompting technique---CoT---is a causal contributor.

\para{Language of thought and diversity.}
Most closely related to our work, \citet{xu2026language} demonstrated that standard English-language CoT creates ``convergence basins'' in thinking space within individual models. By controlling the thinking language via translated prefixes, they showed that non-English reasoning yields 5.3--7.7 point improvements in output diversity, and that languages geometrically farther from English in representation space produce greater diversity. Their study focuses on diversity within a single model across different thinking languages. We complement their findings by measuring convergence \emph{across} model families under the same language (English), and find that CoT's homogenizing effect extends to the cross-model setting.

\para{Faithfulness of CoT reasoning.}
Several studies have shown that CoT explanations can be systematically unfaithful. \citet{turpin2023language} demonstrated that models produce confident CoT explanations that never mention biasing features, even when those features determine the answer. \citet{chen2025reasoning} extended this to reasoning models (Claude 3.7, DeepSeek R1), finding faithfulness rates of only 25--39\%. \citet{afzal2025knowing} showed that models encode information about CoT success \emph{before} generating any reasoning tokens, suggesting that CoT may be post-hoc rationalization rather than genuine computation. These findings suggest a mechanism for our observed convergence: if CoT is partially a surface-level rationalization, models may converge on shared rationalization templates drawn from similar training data, even when their underlying computations differ.